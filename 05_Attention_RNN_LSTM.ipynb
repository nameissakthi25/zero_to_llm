{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism for Sequence Models\n",
    "\n",
    "This notebook implements:\n",
    "- Attention mechanism from scratch\n",
    "- RNN with Attention\n",
    "- LSTM with Attention\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "Attention allows the model to \"look back\" at all previous hidden states and focus on the most relevant ones.\n",
    "\n",
    "```\n",
    "Without Attention:              With Attention:\n",
    "h_0 → h_1 → h_2 → h_3 → y       h_0 → h_1 → h_2 → h_3\n",
    "                     ↑           ↓     ↓     ↓     ↓\n",
    "              (only h_3)         [weighted combination]\n",
    "                                          ↓\n",
    "                                          y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 46, Train: 133716, Val: 14858\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "path = \"data/alice.txt\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "\n",
    "data = np.array([char2idx[ch] for ch in text], dtype=np.int32)\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, Train: {len(train_data)}, Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def softmax(x):\n",
    "    x = np.asarray(x).flatten()\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def softmax_2d(x):\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def one_hot(idx, size):\n",
    "    vec = np.zeros((size, 1))\n",
    "    vec[idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "def xavier(fan_in, fan_out):\n",
    "    std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.randn(fan_out, fan_in) * std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RNN with Attention\n",
    "\n",
    "## Attention Mechanism\n",
    "\n",
    "```\n",
    "score(h_t, h_s) = v^T × tanh(W_a × h_t + U_a × h_s)\n",
    "α = softmax(scores)\n",
    "context = Σ α_i × h_i\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN + Attention initialized. H=256, A=128\n"
     ]
    }
   ],
   "source": [
    "# RNN + Attention Parameters\n",
    "H = 256  # Hidden size\n",
    "A = 128  # Attention size\n",
    "\n",
    "# RNN parameters\n",
    "W_xh = xavier(vocab_size, H)\n",
    "W_hh = xavier(H, H)\n",
    "b_h = np.zeros((H, 1))\n",
    "\n",
    "# Attention parameters - NOTE: v_a is 1D for dot product\n",
    "W_a = xavier(H, A)    # (A, H)\n",
    "U_a = xavier(H, A)    # (A, H)\n",
    "v_a = np.random.randn(A) * 0.01  # (A,) - 1D vector!\n",
    "\n",
    "# Combination layer\n",
    "W_c = xavier(2 * H, H)\n",
    "b_c = np.zeros((H, 1))\n",
    "\n",
    "# Output layer\n",
    "W_y = xavier(H, vocab_size)\n",
    "b_y = np.zeros((vocab_size, 1))\n",
    "\n",
    "print(f\"RNN + Attention initialized. H={H}, A={A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(h_t, hidden_states, t):\n",
    "    \"\"\"Compute attention context vector using Bahdanau attention.\"\"\"\n",
    "    if t == 0:\n",
    "        return np.zeros((H, 1)), np.array([])\n",
    "    \n",
    "    scores = []\n",
    "    Wh = W_a @ h_t  # (A, 1)\n",
    "    \n",
    "    for s in range(t):\n",
    "        h_s = hidden_states[s]\n",
    "        Uh = U_a @ h_s  # (A, 1)\n",
    "        tanh_out = np.tanh(Wh + Uh)  # (A, 1)\n",
    "        # v_a is (A,), tanh_out.flatten() is (A,) -> scalar\n",
    "        score = np.dot(v_a, tanh_out.flatten())\n",
    "        scores.append(score)\n",
    "    \n",
    "    attention_weights = softmax(np.array(scores))\n",
    "    \n",
    "    context = np.zeros((H, 1))\n",
    "    for s in range(t):\n",
    "        context += attention_weights[s] * hidden_states[s]\n",
    "    \n",
    "    return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_attention_forward(inputs, targets, h_prev):\n",
    "    \"\"\"Forward pass for RNN with Attention.\"\"\"\n",
    "    T = len(inputs)\n",
    "    xs, hs, contexts, alphas, combined, ps = {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = h_prev\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        xs[t] = one_hot(inputs[t], vocab_size)\n",
    "        hs[t] = np.tanh(W_xh @ xs[t] + W_hh @ hs[t-1] + b_h)\n",
    "        contexts[t], alphas[t] = compute_attention(hs[t], hs, t)\n",
    "        h_context = np.vstack([hs[t], contexts[t]])\n",
    "        combined[t] = np.tanh(W_c @ h_context + b_c)\n",
    "        ys = W_y @ combined[t] + b_y\n",
    "        ps[t] = softmax_2d(ys)\n",
    "        loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
    "    \n",
    "    return loss, (xs, hs, contexts, alphas, combined, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_attention_backward(inputs, targets, cache):\n",
    "    \"\"\"Backward pass for RNN with Attention.\"\"\"\n",
    "    xs, hs, contexts, alphas, combined, ps = cache\n",
    "    T = len(inputs)\n",
    "    \n",
    "    dW_xh, dW_hh, db_h = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(b_h)\n",
    "    dW_a, dU_a, dv_a = np.zeros_like(W_a), np.zeros_like(U_a), np.zeros_like(v_a)\n",
    "    dW_c, db_c = np.zeros_like(W_c), np.zeros_like(b_c)\n",
    "    dW_y, db_y = np.zeros_like(W_y), np.zeros_like(b_y)\n",
    "    \n",
    "    dh_next = np.zeros((H, 1))\n",
    "    dh_attention = {t: np.zeros((H, 1)) for t in range(T)}\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        dy = ps[t].copy()\n",
    "        dy[targets[t]] -= 1\n",
    "        dW_y += dy @ combined[t].T\n",
    "        db_y += dy\n",
    "        \n",
    "        dcombined = W_y.T @ dy\n",
    "        dcombined_raw = dcombined * (1 - combined[t]**2)\n",
    "        dW_c += dcombined_raw @ np.vstack([hs[t], contexts[t]]).T\n",
    "        db_c += dcombined_raw\n",
    "        \n",
    "        dh_context = W_c.T @ dcombined_raw\n",
    "        dh_from_combine = dh_context[:H]\n",
    "        dcontext = dh_context[H:]\n",
    "        \n",
    "        if t > 0 and len(alphas[t]) > 0:\n",
    "            for s in range(t):\n",
    "                dh_attention[s] += alphas[t][s] * dcontext\n",
    "            Wh = W_a @ hs[t]\n",
    "            for s in range(t):\n",
    "                Uh = U_a @ hs[s]\n",
    "                tanh_out = np.tanh(Wh + Uh)\n",
    "                dtanh = (1 - tanh_out**2)\n",
    "                scale = alphas[t][s] * np.mean(np.abs(dcontext))\n",
    "                dv_a += scale * tanh_out.flatten()\n",
    "                dW_a += scale * (dtanh @ hs[t].T)\n",
    "                dU_a += scale * (dtanh @ hs[s].T)\n",
    "        \n",
    "        dh = dh_from_combine + dh_next + dh_attention[t]\n",
    "        dh_raw = (1 - hs[t]**2) * dh\n",
    "        dW_xh += dh_raw @ xs[t].T\n",
    "        dW_hh += dh_raw @ hs[t-1].T\n",
    "        db_h += dh_raw\n",
    "        dh_next = W_hh.T @ dh_raw\n",
    "    \n",
    "    for g in [dW_xh, dW_hh, db_h, dW_a, dU_a, dW_c, db_c, dW_y, db_y]:\n",
    "        np.clip(g, -5, 5, out=g)\n",
    "    np.clip(dv_a, -5, 5, out=dv_a)\n",
    "    \n",
    "    return (dW_xh, dW_hh, db_h, dW_a, dU_a, dv_a, dW_c, db_c, dW_y, db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rnn_attention(seed, length=100, temperature=0.8):\n",
    "    h = np.zeros((H, 1))\n",
    "    hs_history = {}\n",
    "    output = seed\n",
    "    \n",
    "    for i, ch in enumerate(seed):\n",
    "        if ch in char2idx:\n",
    "            x = one_hot(char2idx[ch], vocab_size)\n",
    "            h = np.tanh(W_xh @ x + W_hh @ h + b_h)\n",
    "            hs_history[i] = h.copy()\n",
    "    \n",
    "    t = len(seed)\n",
    "    idx = char2idx[seed[-1]]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = one_hot(idx, vocab_size)\n",
    "        h = np.tanh(W_xh @ x + W_hh @ h + b_h)\n",
    "        hs_history[t] = h.copy()\n",
    "        context, _ = compute_attention(h, hs_history, t)\n",
    "        h_context = np.vstack([h, context])\n",
    "        comb = np.tanh(W_c @ h_context + b_c)\n",
    "        y = W_y @ comb + b_y\n",
    "        y = y / temperature\n",
    "        p = softmax_2d(y)\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output += idx2char[idx]\n",
    "        t += 1\n",
    "        if t > 200:\n",
    "            hs_history = {k-100: v for k, v in hs_history.items() if k > t-100}\n",
    "            t = 100\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN + Attention...\n",
      "Iter 0, Loss: 191.4337\n",
      "Sample: alice ub_ajq(ahj ';vgxfz-a)-_\"h\n",
      "[i l,mh;3x],(ky_)z; uarxr;sqctdp)p3`i `jvjhi:s``]e\n",
      "-sl;;?c;ht!_`]hnv(jh[v0\n",
      "--------------------------------------------------\n",
      "Iter 5000, Loss: 93.6090\n",
      "Sample: alice the ponstily tee mous the mock turtle antce.\n",
      "\n",
      "  `what all sere ters turtle.\n",
      "\n",
      "  `coure the mouse dral\n",
      "--------------------------------------------------\n",
      "Iter 10000, Loss: 81.0995\n",
      "Sample: alice the mour her was all dis so one naid the queen avouth the dollowing ou\n",
      "heme was a little gettles, `w\n",
      "--------------------------------------------------\n",
      "Iter 15000, Loss: 75.0201\n",
      "Sample: alice it a great teat:  `i sigh to to a\n",
      "very happensing answerl.  `that the who mead, her heartle poome, i\n",
      "--------------------------------------------------\n",
      "Iter 20000, Loss: 72.2987\n",
      "Sample: alice `but\n",
      "armally told now the reppen of this wele was sime sever wrod to the was sit here to\n",
      "iningull de\n",
      "--------------------------------------------------\n",
      "Iter 25000, Loss: 70.3546\n",
      "Sample: alice ighe of in the pucked make out of all her head made out of turry of lessone\n",
      "of the serth.\n",
      "\n",
      "  it was \n",
      "--------------------------------------------------\n",
      "Iter 30000, Loss: 68.5179\n",
      "Sample: alice                                                                                                     \n",
      "--------------------------------------------------\n",
      "Iter 35000, Loss: 65.4884\n",
      "Sample: alice (it in this way; she was gurtier very good on treed up somet were chat in the facted.\n",
      "\n",
      "     *       \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training RNN + Attention\n",
    "T = 50\n",
    "lr = 0.001\n",
    "num_iterations = 50000\n",
    "\n",
    "adam_cache = {}\n",
    "def adam_update(params, grads, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    for i, (p, g) in enumerate(zip(params, grads)):\n",
    "        if i not in adam_cache:\n",
    "            adam_cache[i] = {\"m\": np.zeros_like(g), \"v\": np.zeros_like(g), \"t\": 0}\n",
    "        adam_cache[i][\"t\"] += 1\n",
    "        t = adam_cache[i][\"t\"]\n",
    "        m, v = adam_cache[i][\"m\"], adam_cache[i][\"v\"]\n",
    "        m[:] = beta1*m + (1-beta1)*g\n",
    "        v[:] = beta2*v + (1-beta2)*(g**2)\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        p -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "h_prev = np.zeros((H, 1))\n",
    "pointer = 0\n",
    "smooth_loss = -np.log(1.0/vocab_size) * T\n",
    "loss_history_rnn = []\n",
    "\n",
    "print(\"Training RNN + Attention...\")\n",
    "for iteration in range(num_iterations):\n",
    "    if pointer + T + 1 >= len(train_data):\n",
    "        pointer = 0\n",
    "        h_prev = np.zeros((H, 1))\n",
    "    \n",
    "    inputs = train_data[pointer:pointer+T]\n",
    "    targets = train_data[pointer+1:pointer+T+1]\n",
    "    \n",
    "    loss, cache = rnn_attention_forward(inputs, targets, h_prev)\n",
    "    grads = rnn_attention_backward(inputs, targets, cache)\n",
    "    \n",
    "    params = [W_xh, W_hh, b_h, W_a, U_a, v_a, W_c, b_c, W_y, b_y]\n",
    "    adam_update(params, grads, lr)\n",
    "    \n",
    "    _, hs, _, _, _, _ = cache\n",
    "    h_prev = hs[T-1].copy()\n",
    "    pointer += T\n",
    "    \n",
    "    smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    loss_history_rnn.append(smooth_loss)\n",
    "    \n",
    "    if iteration % 5000 == 0:\n",
    "        print(f\"Iter {iteration}, Loss: {smooth_loss:.4f}\")\n",
    "        print(\"Sample:\", generate_rnn_attention(\"alice \", 100, 0.7))\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Attention Parameters\n",
    "H_lstm = 256\n",
    "A_lstm = 128\n",
    "\n",
    "W_xi, W_hi, b_i = xavier(vocab_size, H_lstm), xavier(H_lstm, H_lstm), np.zeros((H_lstm, 1))\n",
    "W_xf, W_hf, b_f = xavier(vocab_size, H_lstm), xavier(H_lstm, H_lstm), np.ones((H_lstm, 1))\n",
    "W_xo, W_ho, b_o = xavier(vocab_size, H_lstm), xavier(H_lstm, H_lstm), np.zeros((H_lstm, 1))\n",
    "W_xg, W_hg, b_g = xavier(vocab_size, H_lstm), xavier(H_lstm, H_lstm), np.zeros((H_lstm, 1))\n",
    "\n",
    "W_a_l, U_a_l = xavier(H_lstm, A_lstm), xavier(H_lstm, A_lstm)\n",
    "v_a_l = np.random.randn(A_lstm) * 0.01\n",
    "\n",
    "W_c_l = xavier(2*H_lstm, H_lstm)\n",
    "b_c_l = np.zeros((H_lstm, 1))\n",
    "W_y_l = xavier(H_lstm, vocab_size)\n",
    "b_y_l = np.zeros((vocab_size, 1))\n",
    "\n",
    "print(\"LSTM + Attention initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_lstm(h_t, hidden_states, t):\n",
    "    if t == 0:\n",
    "        return np.zeros((H_lstm, 1)), np.array([])\n",
    "    scores = []\n",
    "    Wh = W_a_l @ h_t\n",
    "    for s in range(t):\n",
    "        Uh = U_a_l @ hidden_states[s]\n",
    "        score = np.dot(v_a_l, np.tanh(Wh + Uh).flatten())\n",
    "        scores.append(score)\n",
    "    attn = softmax(np.array(scores))\n",
    "    context = sum(attn[s] * hidden_states[s] for s in range(t))\n",
    "    return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_attention_forward(inputs, targets, h_prev, c_prev):\n",
    "    T = len(inputs)\n",
    "    xs, hs, cs = {}, {}, {}\n",
    "    ig, fg, og, gg = {}, {}, {}, {}\n",
    "    contexts, alphas, combined, ps = {}, {}, {}, {}\n",
    "    hs[-1], cs[-1] = h_prev, c_prev\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        xs[t] = one_hot(inputs[t], vocab_size)\n",
    "        ig[t] = sigmoid(W_xi @ xs[t] + W_hi @ hs[t-1] + b_i)\n",
    "        fg[t] = sigmoid(W_xf @ xs[t] + W_hf @ hs[t-1] + b_f)\n",
    "        og[t] = sigmoid(W_xo @ xs[t] + W_ho @ hs[t-1] + b_o)\n",
    "        gg[t] = np.tanh(W_xg @ xs[t] + W_hg @ hs[t-1] + b_g)\n",
    "        cs[t] = fg[t]*cs[t-1] + ig[t]*gg[t]\n",
    "        hs[t] = og[t]*np.tanh(cs[t])\n",
    "        contexts[t], alphas[t] = compute_attention_lstm(hs[t], hs, t)\n",
    "        hc = np.vstack([hs[t], contexts[t]])\n",
    "        combined[t] = np.tanh(W_c_l @ hc + b_c_l)\n",
    "        y = W_y_l @ combined[t] + b_y_l\n",
    "        ps[t] = softmax_2d(y)\n",
    "        loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
    "    \n",
    "    return loss, (xs, hs, cs, ig, fg, og, gg, contexts, alphas, combined, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_attention_backward(inputs, targets, cache):\n",
    "    xs, hs, cs, ig, fg, og, gg, contexts, alphas, combined, ps = cache\n",
    "    T = len(inputs)\n",
    "    \n",
    "    dW_xi, dW_hi, db_i = [np.zeros_like(x) for x in [W_xi, W_hi, b_i]]\n",
    "    dW_xf, dW_hf, db_f = [np.zeros_like(x) for x in [W_xf, W_hf, b_f]]\n",
    "    dW_xo, dW_ho, db_o = [np.zeros_like(x) for x in [W_xo, W_ho, b_o]]\n",
    "    dW_xg, dW_hg, db_g = [np.zeros_like(x) for x in [W_xg, W_hg, b_g]]\n",
    "    dW_a, dU_a, dv_a = np.zeros_like(W_a_l), np.zeros_like(U_a_l), np.zeros_like(v_a_l)\n",
    "    dW_c, db_c = np.zeros_like(W_c_l), np.zeros_like(b_c_l)\n",
    "    dW_y, db_y = np.zeros_like(W_y_l), np.zeros_like(b_y_l)\n",
    "    \n",
    "    dh_next, dc_next = np.zeros((H_lstm, 1)), np.zeros((H_lstm, 1))\n",
    "    dh_attn = {t: np.zeros((H_lstm, 1)) for t in range(T)}\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        dy = ps[t].copy()\n",
    "        dy[targets[t]] -= 1\n",
    "        dW_y += dy @ combined[t].T\n",
    "        db_y += dy\n",
    "        \n",
    "        dc_raw = W_y_l.T @ dy * (1 - combined[t]**2)\n",
    "        dW_c += dc_raw @ np.vstack([hs[t], contexts[t]]).T\n",
    "        db_c += dc_raw\n",
    "        \n",
    "        dhc = W_c_l.T @ dc_raw\n",
    "        dh_comb, dctx = dhc[:H_lstm], dhc[H_lstm:]\n",
    "        \n",
    "        if t > 0 and len(alphas[t]) > 0:\n",
    "            for s in range(t):\n",
    "                dh_attn[s] += alphas[t][s] * dctx\n",
    "        \n",
    "        dh = dh_comb + dh_next + dh_attn[t]\n",
    "        do = dh * np.tanh(cs[t]) * og[t] * (1-og[t])\n",
    "        dc = dh * og[t] * (1-np.tanh(cs[t])**2) + dc_next\n",
    "        df = dc * cs[t-1] * fg[t] * (1-fg[t])\n",
    "        di = dc * gg[t] * ig[t] * (1-ig[t])\n",
    "        dg = dc * ig[t] * (1-gg[t]**2)\n",
    "        \n",
    "        for dW, dWh, db, d in [(dW_xi,dW_hi,db_i,di), (dW_xf,dW_hf,db_f,df), (dW_xo,dW_ho,db_o,do), (dW_xg,dW_hg,db_g,dg)]:\n",
    "            dW += d @ xs[t].T\n",
    "            dWh += d @ hs[t-1].T\n",
    "            db += d\n",
    "        \n",
    "        dh_next = W_hi.T@di + W_hf.T@df + W_ho.T@do + W_hg.T@dg\n",
    "        dc_next = dc * fg[t]\n",
    "    \n",
    "    grads = [dW_xi,dW_hi,db_i, dW_xf,dW_hf,db_f, dW_xo,dW_ho,db_o, dW_xg,dW_hg,db_g, dW_a,dU_a,dv_a, dW_c,db_c, dW_y,db_y]\n",
    "    for g in grads:\n",
    "        np.clip(g, -5, 5, out=g)\n",
    "    return tuple(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lstm_attention(seed, length=100, temperature=0.8):\n",
    "    h, c = np.zeros((H_lstm, 1)), np.zeros((H_lstm, 1))\n",
    "    hs_hist = {}\n",
    "    output = seed\n",
    "    \n",
    "    for i, ch in enumerate(seed):\n",
    "        if ch in char2idx:\n",
    "            x = one_hot(char2idx[ch], vocab_size)\n",
    "            i_g = sigmoid(W_xi@x + W_hi@h + b_i)\n",
    "            f_g = sigmoid(W_xf@x + W_hf@h + b_f)\n",
    "            o_g = sigmoid(W_xo@x + W_ho@h + b_o)\n",
    "            g_g = np.tanh(W_xg@x + W_hg@h + b_g)\n",
    "            c = f_g*c + i_g*g_g\n",
    "            h = o_g*np.tanh(c)\n",
    "            hs_hist[i] = h.copy()\n",
    "    \n",
    "    t = len(seed)\n",
    "    idx = char2idx[seed[-1]]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = one_hot(idx, vocab_size)\n",
    "        i_g = sigmoid(W_xi@x + W_hi@h + b_i)\n",
    "        f_g = sigmoid(W_xf@x + W_hf@h + b_f)\n",
    "        o_g = sigmoid(W_xo@x + W_ho@h + b_o)\n",
    "        g_g = np.tanh(W_xg@x + W_hg@h + b_g)\n",
    "        c = f_g*c + i_g*g_g\n",
    "        h = o_g*np.tanh(c)\n",
    "        hs_hist[t] = h.copy()\n",
    "        ctx, _ = compute_attention_lstm(h, hs_hist, t)\n",
    "        comb = np.tanh(W_c_l @ np.vstack([h, ctx]) + b_c_l)\n",
    "        y = W_y_l @ comb + b_y_l\n",
    "        p = softmax_2d(y / temperature)\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        output += idx2char[idx]\n",
    "        t += 1\n",
    "        if t > 200:\n",
    "            hs_hist = {k-100: v for k,v in hs_hist.items() if k > t-100}\n",
    "            t = 100\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training LSTM + Attention\n",
    "adam_cache_l = {}\n",
    "def adam_l(params, grads, lr):\n",
    "    for i, (p, g) in enumerate(zip(params, grads)):\n",
    "        if i not in adam_cache_l:\n",
    "            adam_cache_l[i] = {\"m\": np.zeros_like(g), \"v\": np.zeros_like(g), \"t\": 0}\n",
    "        adam_cache_l[i][\"t\"] += 1\n",
    "        t = adam_cache_l[i][\"t\"]\n",
    "        m, v = adam_cache_l[i][\"m\"], adam_cache_l[i][\"v\"]\n",
    "        m[:] = 0.9*m + 0.1*g\n",
    "        v[:] = 0.999*v + 0.001*(g**2)\n",
    "        p -= lr * (m/(1-0.9**t)) / (np.sqrt(v/(1-0.999**t)) + 1e-8)\n",
    "\n",
    "h_prev_l, c_prev_l = np.zeros((H_lstm, 1)), np.zeros((H_lstm, 1))\n",
    "pointer = 0\n",
    "smooth_loss = -np.log(1.0/vocab_size) * T\n",
    "loss_history_lstm = []\n",
    "\n",
    "print(\"\\nTraining LSTM + Attention...\")\n",
    "for iteration in range(num_iterations):\n",
    "    if pointer + T + 1 >= len(train_data):\n",
    "        pointer = 0\n",
    "        h_prev_l, c_prev_l = np.zeros((H_lstm, 1)), np.zeros((H_lstm, 1))\n",
    "    \n",
    "    inputs = train_data[pointer:pointer+T]\n",
    "    targets = train_data[pointer+1:pointer+T+1]\n",
    "    \n",
    "    loss, cache = lstm_attention_forward(inputs, targets, h_prev_l, c_prev_l)\n",
    "    grads = lstm_attention_backward(inputs, targets, cache)\n",
    "    \n",
    "    params = [W_xi,W_hi,b_i, W_xf,W_hf,b_f, W_xo,W_ho,b_o, W_xg,W_hg,b_g, W_a_l,U_a_l,v_a_l, W_c_l,b_c_l, W_y_l,b_y_l]\n",
    "    adam_l(params, grads, lr)\n",
    "    \n",
    "    _, hs, cs, _, _, _, _, _, _, _, _ = cache\n",
    "    h_prev_l, c_prev_l = hs[T-1].copy(), cs[T-1].copy()\n",
    "    pointer += T\n",
    "    \n",
    "    smooth_loss = 0.999*smooth_loss + 0.001*loss\n",
    "    loss_history_lstm.append(smooth_loss)\n",
    "    \n",
    "    if iteration % 5000 == 0:\n",
    "        print(f\"Iter {iteration}, Loss: {smooth_loss:.4f}\")\n",
    "        print(\"Sample:\", generate_lstm_attention(\"alice \", 100, 0.7))\n",
    "        print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history_rnn, label='RNN + Attention', alpha=0.8)\n",
    "plt.plot(loss_history_lstm, label='LSTM + Attention', alpha=0.8)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal RNN+Attn Loss: {loss_history_rnn[-1]:.2f}\")\n",
    "print(f\"Final LSTM+Attn Loss: {loss_history_lstm[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "print(\"=\"*60)\n",
    "print(\"RNN + Attention:\")\n",
    "print(generate_rnn_attention(\"alice \", 200, 0.6))\n",
    "print(\"\\nLSTM + Attention:\")\n",
    "print(generate_lstm_attention(\"alice \", 200, 0.6))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "| Model | Expected Loss |\n",
    "|-------|---------------|\n",
    "| Vanilla RNN | ~64 |\n",
    "| Vanilla LSTM | ~39 |\n",
    "| RNN + Attention | ~55-60 |\n",
    "| LSTM + Attention | ~35-40 |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. Attention allows looking back at all previous hidden states\n",
    "2. Context = weighted sum of past states based on relevance\n",
    "3. Bahdanau attention: `score = v^T × tanh(W×h_t + U×h_s)`\n",
    "4. Attention is the foundation for Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
